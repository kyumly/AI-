# AI-
AI공부
혼자 공부하는 머신러닝+딥러닝 실습

# 1장 머신러닝이란?

데이터를 학습시켜 기준을 세우는것 
ex) ML하기전 코드 : if x < 30
    ML 한 후 : 데이터를 가지고 기준을 찾음

***
# 2장 데이터 다루기
## 2.1훈련 세트와 테스트 세트  
### 지도 학습과 비지도 학습
    머신 러닝 알고리즘은 크게 2가지로 지도 학습, 비지도 학습으로 나눌수 있음
    지도 학습은 : 입력(input), 타킷(target) 
    비지도 학습 : 입력(input)
    
    특성, 속성 : 각각에 레이블(클래스)들의 특징을 뽑아서 학습 모델에 정답에 도출하는 데이터, 특징이 많으면, 클래스을 파악하기 쉬움
    ex) 
    라벨, 클래스 : 특징들에 대한 결과물

### 훈련 세트와 테스트 세트
    훈련 세트, 테스트 세트 : 모델을 훈련할 때 훈련 세트를 사용하고, 평가는 테스트 세트로 함
    데이터들을 분리 하는 이유는 샘플링 편향, 과대 적합, 과소 적합에 대한 논쟁이 있음   
### 샘플링 편향
    샘플링 편향이란 훈련, 테스트 데이터들이 골고루 섞여야 한다. 만약에 안 섞이면, 한곳에 데이터들이 몰려, 제대로 된 훈련하지 못한다.
    
## 2.2 데이터 전처리
    
### 데이터 전처리를 하는 이유?
1. ML를 하는데 가장 중요한 것은 데이터이다. 만약 데이터가 null, 부정확한 값이 있다면 등, 제대로 된 기계 학습을 할 수 없다. 그리고 데이터편향을 방지 하기위해, 데이터도 적절하게 석어야 한다.<>
2. 샘플들의 특성을 파악해 클래스들간에 상관관계를 파악해야한다. 필요 없는 특성은 학습하는 전에 빼야 한다.
3. 특성값을 일정한 기준으로 맞춰야 한다. 이런 작업들을 데이터 전처리라고 한다.

### 기준 맞추기
1. 데이터를 표현 방식을 맞춰야 한다. 만약에 x축, y축간에 스케일이 맞지 않는다면, 정확한 데이터 분석이 불가능 하다. 그래서 스케일을 맞추기 위해 데이터 정규화 과정을 해야한다.
2. 데이터 정규화에는 2가지 방식이 있다. 범위 변환, Z 변환이 있다. 범위 변환은 정규화후 최소값을 0, 최대값을 1로, 'Z 변환 '은 정규화 후의 변수 평균값이 0, 표준편차가 1이 되도록 변환한다. (정규화 : 범위변환, 표준화 : Z변환)
3. MinMaxScaler -> 범위 변환, StandardScaler -> Z 변환
4. 사용 방법 : 패키지 import -> mc = import한 값 -> mc.transform(데이터 프레임)
#### 표준편차 vs 표준점수
    분산은 데이터에서 평균을 뺀 값을 모두 제곱한 다음 평균을 내어 구함, 표준 편차는 분산의 제곱근으로 데이터가 분산된 정도를 나타낸다.
    표준 점수는 데이터가 원점에서 몇 표준편차만큼 떨어져 있는지를 나타내는 값
        
***
# 3장 회귀 알고리즘과 모델 규제
## 3.1 K-최근접 이웃 회귀
### k-최근접 이웃 회귀란?
지도 학습 알고리즘은 크게 분류와 회귀로 나눈다.<br>
회귀 : 클래스 중 하나로 분류하는 것이 아니라 임의의 **어떤 숫자를 분류**하는 문제<br>
분류 : 클래스 중 하나를 **분류**하는 문제<hr>
    ex) 농어의 길이를 가지고 농어에 무게를 파악
    ex) 길이, 무게, 높이를 가지고 농어인지, 방어인지 분류
<hr>
    K-최근접 이웃회귀 : 예측하려는 샘플에 가장 가까운 샘플 K개를 선택해, 이 샘플들의 클래스를 확인하여 다수 클래스를 새로운 샘플의 클래스로 예측한다.<br>
    데이터에 대한 이해 : sklearn에서는 모든 하이퍼매개변수를 행렬(2차원 배열)형태로 받아야 한다. (데이터베이스 형태)<br>
    ex) test_array = test_array.reshape(-1,1) 1개의 특성을 가지는 행렬

### 결정계수(R^2)
 모델.score() 입력하면, 해당 클래스에 대한 점수가 나온다. 분류일 경우 분류 확률로 표시되고,<br> 
 회귀는 결정계수(R^2)라고 부른다.<br>
 결정 계수란 회귀에서 정확한 값을 측정하는것은 힘들다. 즉 예측값에 대한 정확도로 알 수 있다.<br>
 R^2 = 1 - (타킷-예측)^2 합 / (타킷-평균)^2 합 계산된다. <br>
 0 ~ 1 값으로 나타난다.
### 과대 적합 vs 과소 적합
과대 적합 : 훈련 세틍만 잘 맞는 모델이라 테스트 세트와 나중에 실전에 투입하야 새로운 샘플에 대한 예측을 만들 때 잘 동작하지 않는다.<br>
과소 적합 : 훈련 세트보다 테스트 세트의 점수가 높거나 두 점수가 모두 너무 낮은 결우 과소 접합이라 한다.


## 3.2선형 회귀
### K-최근접 이웃의 한계
모델이 예측했던 값과 정확히 일치하지만, K-최근접 이웃 회귀는 가장 가까운 샘플을 찾아 타킷에 평균을 구하기 때문에, 새로운 샘플이<br>
훈련 세트의 범위를 벗어나면 엉뚱한 값을 예측한다 -> 해결 방법으로는 선형회귀를 사용한다.

### 선형 회귀
특성이 하나인 경우 어떤 직선을 학습하는 알고리즘 -> 하나의 특성을 추출하는것이 제일 중요함<br>
y = ax + b 
a : 기울기(가중치), b : y절편, x : 특성, y : 라벨<br>
lr.coef_(기울기), lr.intercept_(절편)
### 다항회귀
선형회귀에 있던 일차함수 형식에서 n차 함수식으로 변경하는 기법이다. 주의할 점으로는 다중 회귀랑 헷갈리면 안된다. 다중 회귀는 여러개의 특성을 가지고 회귀를 분석하는 역할을 하지만,<br>
다항회귀는 하나의 특성을 변형시켜 모델을 훈련시키는 기법이다.<br>
[다중/다항 회귀에 대한 설명](https://dodonam.tistory.com/236) - 데이터과학님 설명!!
***

## 3.3 특성 공학과 규제
### 다중 회귀
다중 회귀는 하나의 특성을 사용하는 것이 아니라, N개의 특성을 가지고 모델을 학습하는 알고리즘이다.
### 특성공학
    기존의 특성을 사용해 새로운 특성을 뽑아내는 작업을 특성공학이라고 한다. 사이키런에서 제공하고 있는 polynomialFeatures를 사용해 특성을 여러개 만든다.

### 규제
1. 규제는 머신러닝 모델이 훈련 세트를 너무 과도하게 학습하지 못하도록 훼방하는 것을 말한다. 즉 과대 적합이 되지 않도록 만드는 것이다.
2. 규제를 하기전에 정규화를 통해 모든 값들에 대한 scaler 조정해야한다. 정규화를 통해 변환을 시도한다. (StandarScaler클래스 사용) 
3. 선형 회귀 모델에 규제를 추가한 모델을 릿지와 라쏘라고 한다.

### 릿지 회귀 vs 라쏘 
1. 릿지 : 계수를 제곱한 값으로 규제를 실행한다. 
2. 라쏘 : 계수의 절대값을 기준으로 실행한다.
3. 공통점 : alpha값 사용하여 계수를 줄이는 역할을 실행한다.<br>
[참고사이트](https://rk1993.tistory.com/entry/Ridge-regression%EC%99%80-Lasso-regression-%EC%89%BD%EA%B2%8C-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0) - 우주먼지의 하루님!!
***
# 4장 다양한 분류 알고리즘
## 4.1 로지스틱 회귀
### 러키백의 확률
1. 랜덤박스 안에 여러가지의 생선들이 있다. 분류 알고리즘을 하기 위해 첫번째 최근접 알고리즘을 사용했다.
2. 최근접 알고리즘을 사용하면, 전체에 대한 문제가 발생한다. 근접한 클래스만 표본이 되기 때문에 전체에 대한 문제가 발생한다.
3. 3개의 최근접이라면, 0/3, 1/3, 2/3, 3/3 발생.
### 로지스틱 회궈
로지시틱 회귀는 이름은 회귀지만, 선형 회귀와 동일하게 선형 방정식을 학습한다.
z = ax + by + cz + ... + f 로지스틱 회귀에 방정식에 대한 가중치 값이 가장 중요하다.
z값은 아무값이나 가능하지만, 분류 확률을 파악하기 위해서는 0 ~ 1 사이값으로 변해야 한다.
z값 사용하기 위해 시그모이드(로지스틱)함수를 사용하여 변경이 가능하다. 
0.5 보다 작으면 음성, 0.5보다 크면 양성 클래스로 분류된다.
### 로지스틱 다중 분류
    다중 분류에서 중요한것은 각 클래스 별로 가중치와 절편이 있다.
## 4.2 확률적 경사 하강법 <- 다시 공부하기
### 점진적인 학습
    만약에 새로운 데이터들이 들어온다면 어떡게 처리해야 할것인가?
1. 해결 방법 생각
    1. 데이터가 추가될 때마다 새로운 훈련을 시킬것인가?
    2. 아님 이전에 있는 훈련데이터를 버림으러써 일정량에 훈련데이터를 유지할 것인가?
2. 결론
   1. 앞서 훈련한 모델을 버리지 않고, 새로운 데이터에 대해서만 조금씩 더 훈련
   2. 점진적 합습 또는 온라인 학습으로 해결
   3. 대표적인 점진적 합습은 확률적 경사 하강법

### 확률적 경사 하강법
    확률적 경사 하강법을 단어 하나식 쪼개서 생각하면 확률적 -> 무작위 하게, 경사 -> 가파른 언덕, 하강 -> 내려오다.
즉 하나로 합치면 가파른 언덕에서 무작위하게 내려오는 형식을 뜻한다.<br>
1. 경사 하강법 이란?
   1. 경사를 따라 내려온다
   2. 가장 가파른 길을 찾아 내려오지만, 조금씩 내려오는것이 중요하다.
2. 확률적이란 ?
   1. 랜덤하게 훈련세트에서 하나의 샘플만 사용한다.
3. 결론
   1. 훈련세트에서 랜덤하게 하나의 샘플을 선택하여 가파른 경사를 조금씩 내려간다.
   2. 그 다움에 또 훈련세트에서 랜덤하게 하나의 샘플을 선택하여 조금씩 내려간다.
   3. 샘플을 다 사용 했는데 산을 하강하지 못했다면, 훈련 세트에 모든 샘플을 다시 채우고 반복한다.
   4. 만족하는 위치에 도달 할 때까지 반복한다.
   5. 훈련 세트를 한 번 모두 사용하는 과정을 에포크(epoch)라고 부른다.


### 배치, 미니, 확률적 경사 하강법 용어 정리
1. 배치 경사 하강법 : 극단적으로 한 번 경사로를 따라 이동하기 위해 **전체 샘플**을 사용
2. 미니배치 경사 하강법 : **여러개의** 샘플을 사용해 경사 하강법을 수행
3. 확률적 경사 하강법 : **하나씩** 샘플을 사용해 경사 하강법을 수행

### 손실 함수
    어떤 문제에서 머신러닝 알고리즘이 얼마나 엉터리인지를 측정하는 기준이다.
1. 정확도 변에서 단점 발생
   1. 샘플이 4가지면 0, 25%, 50%, 75% 100%
   2. 경사 하강법은 조금씩 내려간다 했지만, 지금 확률에 폭이 크다
   3. 정확도가 폭이 크면 경사 하상법을 이용해 사용할 수 없다.
   4. 즉, 미분이 가능해야한다.


### 로지스틱 손실 함수
    -log함수안에 0 ~ 1 값을 파악해서 1이랑 가까울 수록 좋은것!

***
5장 트리 알고리즘
## 5.1 결정트리
### 로지스틱 회귀로 와인 뷴류
1. 로지스틱 회귀로 와인 분류를 하면 시그모이드 함수를 통해 이진 분류를 실행한다.
2. 예측은 할 수 있지만, 설명 하기 쉬운 모델 vs 어려운 모델에 대한 고민
3. 로지스틱 회귀에 나오는, 가중치, 절편에 대한 설명은 하기가 어렵다

### 결정 트리
    결정 트리는 스무고개와 같다, YES / NO에 대한 선택에 따라 하향식으로 내려가게 된다.
1. 결정 트리는 질문을 통해 최적에 답을 찾아 간다.
2. node 종류
   1. root node(최상위 노드)
   2. leaf node(리프 노드)
3. node안에 조건
   1. 테스트 조건 : 조건이 맞으면 왼쪽 / 틀리면 오른쪽
   2. 불순도 : 데이터를 분할할 기준을 정하는 것
   3. 총 샘플 수 : 총 샘플 수
   4. 클래스 별 샘플 수 :  [x, y] 양성, 음성 구별 갯수 

### 지니 불순도 
    지니 불순도 = 1 - (음성 클래스 비율^2 + 양성 클래스 비율^2)
1. 부모 노드와 자식 노드의 불순도 차이가 가능한 크도록  트리를 성장 시킴.<br>
계산 방법 : 부모의 불순도 - (왼쪽 노드 샘플 수/ 부모의 샘플 수) * 왼쪽 노드 불순도 - (오른쪽 노드 샘플 수/ 부모의 샘플수) * 오른쪽 노드 불순도 =결과
2. 부모와 자식 노드 사이의 불순도 차이를 정보 이득이라고 한다.

### 엔트로피 불순도
    -음성 클래스 비율 * log2(음성 클래스 비율) - 양성 클래스 비율 * log2(양성 클래스 비율)
### 가지 치기
    가지 치기 : 과대 적합을 방지하기 위해
1. max_depth 매개변수를 설정하면, 노드가 최대 3개의 노드까지만 성장 한다.
2. tree 노드는 표준화 전처리를 할 필요가 없다.
3. dt.feature_importances_ 메소드를 사용해, 속성 값을 출력한다.



## 5.2 교차 검증과 그리드 서치
## 검증 세트 
    테스트 세트를 사용하지 않으면 모델이 과대적합인지 과소 적합인지 판단하기 어렵다.
1. 테스트 세트를 사용하지 않고, 훈련세트를 나눠서 검증 세트를 만들어서 사용한다.
2. 훈련세트 60/ 검증 20/ 테스트 20

## 교차 검증
    검증 세트를 train_test_split를 사용하면, 많은 양에 훈련 데이터를 할 수 없다.
1. 교차 검증을 사용하면 안정적인 검증 점수를 얻고 훈련에 더 많은 데이터를 사용할 수 있다.
2. K-FOLD 방식을 통해 훈련 세트를 동일하게 나눠서, K번 실행하는 방법이다.
3. 그리고 각 각의 검증 점수를 평균하기 때문에 안정된 점수라고 할 수 있다.
4. 용어 정리
   1. fit_time : 모델을 훈련 하는 시간
   2. score_time : 검증 하는 시간
   3. test_score : 교차 검증 점수
5. print(np.mean(scores['test_score'])) 통해 평균 제시
6. cv : 분할기
## 하이퍼 파라미터 튜닝
    하이퍼 파라미터는 사용자가 매개변수를 설정해 모델을 학습 시킨다.
1. 최적의 하이퍼 파리마터를 찾기 위해서는 그리드서치 모델을 사용한다.
2. GridSearchCv(모델, 파라미터, cpu코어수 지정)
3. 용어
   1. gs.cv_results_ : 총 결과
   2. gs.best_params_ : 최적의 파라미터

## 랜덤 서치
    매개 변수의 값이 수치일 때 값의 범위나, 간격을 미리 정하기 어려움, 
    또 많은 매개 변수를 있으면 수행 시간이 오래 걸릴수 있음
1. 해결 하기 위해 랜덤하게 설정해 찾아간다.
## 5.3트리의 앙상블

***
# 6장 비지도 학습 
## 6.1 군집 알고리즘
### 타킷을 모르는 비지도 학습그
    타킷이 없을 때 사용하는 머신러닝 알고리즘이 비지도 학습 기반
### 그림에 대한 데이터 전처리
1. 그림은 3차원으로 나타낸다. (300, 100, 100) -> 100 X 100 이미지크기를 300개 가지고 있다.
2. 이미지 출력은 imshow() 메소드 사용
3. subplots() 함수를 사용하면, 여러개의 그래프를 배열처럼 쌓을 수 있다.
### 픽셀값 분석하기
    다차원 배열보다 1차월 배열이 계산할 때 편리하다, 하지만 출력에는 어려움이 있다.
1. apple = fruits[0:100].reshape(-1, 100*100)
2. 100개의 사과 사진을, 10000개의 특성을 나타냄
3. axis에 대한 이해
   1. axis = 0 : 행 기준
   2. axis = 1 : 열 기준
4. 히스토그램 : 값이 발생한 빈도를 그래프로 표시한 것.
5. 평균 값을 가지고, 이미지 출력하기

## 6.2 K-평균
### K - 평균 알고리즘 소개
   1. 무작위로 K개의 클러스터 중심을 정한다..
   2. 각 샘플에서 가장 가까운 클러스터 중심을 찾아 해당 클래스터의 샘플로 지정한다.
   3. 클러스터에 속한 샘플의 평균값으로 클러스터 중심을 변경한다.
   4. 클러스터 중심에 변화가 없을 때 까지 2번으로 돌아가 반복한다.

### KMeans 클래스
1. n_clusters 분류에 집단을 설정한다.
2. km.labels_ : 분류에 대한 결과값이 나온다.

### 최적의 K 찾기
   K-평균 알고리즘의 단점은, 클러스터 개수를 사전에 지정해야 한다는 단점
1. 해결 하는 방법은 **엘보우** 사용
2. K-평규 알고리즘은 클러스터 중심과 클러스터에 속한 샘플 사이의 거리를 잴 수 있음
3. 이 거리의 제곱 합을 이너셔이라고 부른다.

## 6.3 주성분 분석 <- 나중에

***
# 7장 딥러닝을 시작합니다.
##패션 MNIST
      머신러닝 딥러닝을 공부할 때 많이 사용하는 데이터 셋!
1. train : 28 X 28 사이즈를 가진 60,000개의 데이터
2. test :  28 X 28 사이즈를 가진 10,000개의 데이터
3. print(np.unique(train_target, return_counts=True))
   1. 중복을 제거하고, 타켓 값을 가져온다.
   2. 각 타켓에 대한 갯수를 카운트 해서 알려준다
### 로지스틱 회귀로 패션 아이템 분류하기
      로지스틱회귀 생각하면, 각 각에 가중치 값과 특성 값을 곱해 z 값을 구해
      시그모이드 함수로 0 ~ 1 사이값으로 분류 실행
다중 로지스틱 회귀일 경우, 각 각의 클래스당 특성들이 있음
1. 신발 : a*x + b*y + c*z + f
2. 바지 : a*x + b*y + c*z + f
3. 인공신경망과 유사한 구조

###인공 신경망
1. 출력층 : z1 ~ z10을 계산하고 이를 바탕으로 클래스를 예측하기 때문에 신경망의 최종 값을 만든다는 의미에서 출력층이라고 불려짐
2. Z값을 계산하는 단위를 뉴런이라고 부른다. 유닛이라고 불려지기도 한다.
3. X1 ~ x784(가중치 값)을 입력층이라 한다.
4. 딥러닝은 머신러닝에 알고리즘 중 하나이다.!!!!
### 인공 신경망으로 모델 만들기
1. 밀집층 : 가중치(784)개 선들이 각 각의 뉴런에 연결되어 총 10 * 784개의 선이 생긴다.
2. 완전 연결층 : 양쪽 뉴런이 모두 연결되있는 상태
3. 절편도 뉴런마다 더해 진다.
4. 소프트맥스와 같이 뉴런의 선형 방적식 게산 결과에 적용되는 함수 -> 활성화 함수

### 신경 인공망으로 패션 아이템 분류하기
1. compile : 모델 객체를 만든 후 훈련하기 전에 사용할 손실 함수와 츨정 지표등을 지정하는 메소드
2. loss : 분류에 대한 판다
   1. 이진 분류 : binary_crossentropy
   2. 다중 분류 : categorical_crossentropy
3. sparse 설명 : 
   1. 인터넷에서 찾아보기!!


## 7.2 심층 신경망
### 2개의 층
   1. 은닉층 : 입력층, 출력층 사이에 밀집층이 추가
   2. 활성함수 : 신셩망 층의 선형 방정식의 계산 값에 적용하는 함수
   3. 함수
      1. 이진 분류 : 시그모이드
      2. 다중 분류 :소프트 맥스
      3. 은닉층 활성함수 : 시그모이드, 렐루(Relu)
      4. 회귀일 때는 activation 매개변수에 아무런 값을 지정 하지 않는다.
   4. 은닉층에 활성함수를 적용하는 이유 : 은닉층에서 선형적인 산술 계산만 한다면 수행 역할이 없는 셈이다. 그래서 비 선형적으로 비틀어 줘야한다.
   5. 적어도 출력층보다는 은닉층 뉴런이 더 많아야 한다.

### 심층 신경망 만들기
    model = keras.Sequential([dense1, dense2])
1. 리스트 형태로 보내고, 출력층은 꼭 마지막에 있어야한다.
2. model.summary() 층에 대한 정보를 출력한다.

### 층을 추가하는 다른 방법법
    model = keras.Sequential()
    model.add(keras.layers.Dense(100, activation='sigmoid', input_shape=(784,)))
    model.add(keras.layers.Dense(10, activation='softmax'))
    model.summary()
    model.add() 메소드를 사용해 층을 추가하는 방법
### 렐루 함수
    입력이 양수일 경우 마치 활성화 함수가 없는 것처러 그냥 통과 시키고 
    음수일 경우 0으로 만든다.
1. 렐루 함수는 이미지 처리에 좋은 함수
2. 인공 신경망을 처리하기 위해 1차원 배열 형태로 변경 했지만, 케라스에서는 Flatten층을 제공
3. Flatten 클래스는 배치 차원을 제외하고 나머지 입력 차원을 모두 일렬로 펼치는 역할만 함.


### 옵티마이저
    신경망은 하이퍼파라미터가 특히 많다.
1. 케라스는 기본적으로 미니경사하강법을 사용, 미니배치 개수는 32개
2. fit 메소드의 batch_size 매개변수를 통해 사이즈 조정
3. epochs 매개변수도 변경 가능
4. RMSporop 사용, <- 다양한 경사 하강법 제공
5. 이들을 optimizer(옵티마이저)라고 지칭함

## 7.3 신경망 모델 훈련
### 손실 곡선
    fit안에는 에포크, 손실, 정확도 등이 있다. 흔히 history = model.fit() 선언한다.
1. History 객체에는 훈련 과정에서 계산한 지표, 즉 손실과 정확도 값이 저장되어 있음
2. History 객체를 통해 그래프 표현해, 과대적합, 손실함수 분석등 여러가지를 할 수 있음

### 검증 손실
    fit메소드에 매개변수 validation_data=(,) 추가해준다.

### 드롭 아웃
    드롭 아웃은 훈련 과정에서 층에 있는 일부 뉴런을 랜덤하게 꺼서(즉 뉴런의 출력을 0으로 만든다) 과대적합을 방지
1. 드롭 아웃이 과대적합을 방지하는 방법은 일부의 뉴런이 랜덤하게 꺼지면 특성 뉴런에 과대하게 의존하는 것을 줄일 수 있고 모든 입력에 대해 주의를 기울어야 한다.
2. 어떤 층의 뒤에 드롭다웃을 두어 이 층의 출력을 랜덤하게 0으로 만든다. (층 뒤에 설정한다.)
***
### 모델 저장과 복원
1. save/load = 모델 전체를 파일로 저장하고, 불러오는 방식
2. save/load_weights = 가중치만 저장하고, 가중치만 불러오기
   1. load_weights 사용하기 위해서는 메소드로 저장했던 모델과 정확히 같은 구조를 가져야한다.
3. predict 메소드를 사용해 예측한다.
   1. predict : 이미지 분류 결과를 예측한다.
   2. evaluate : 테스트 이미지 데이터 세트를 입력해서 성능 평가를 한다.
4. argmax : y = f(x), y값을 반환하는 것이 아니라 x값을 반환한다.
# 8장 이미지를 위한 인공 신경망
***
# 9장 텍스트를 위한 인공신경망
***
### 참고문헌
1. 혼자 공부하는 머신러닝
2. 파이 데이터 분석
